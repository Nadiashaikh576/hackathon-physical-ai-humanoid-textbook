# Module 4: Vision-Language-Action (VLA) Humanoid Intelligence

Welcome to Module 4 of the AI-Native Book focused on Physical AI & Humanoid Robotics. This module covers the Vision-Language-Action (VLA) paradigm, explaining how language, vision, and action are unified in intelligent robots. This module teaches how large language models, speech recognition, and perception systems converge to enable humanoid robots to understand human intent and act autonomously in the physical world.

## Overview

This module explores how humanoid robots integrate vision, language, and action to understand and respond to human commands autonomously:

- Voice-to-action conversion using speech recognition
- Cognitive planning using Large Language Models (LLMs)
- End-to-end autonomous humanoid behavior

## Learning Outcomes

By the end of this module, you should be able to:
- Understand the Vision-Language-Action (VLA) paradigm
- Explain how voice commands are converted into robot actions
- Describe cognitive planning using LLMs
- Conceptually understand end-to-end autonomous humanoid behavior

## Prerequisites

- Prior knowledge of ROS 2, simulation, and perception
- Basic understanding of robotics concepts
- Familiarity with Docusaurus documentation navigation

## Structure

The module is organized into three chapters:

1. [Chapter 1: Voice-to-Action — Speech as Robot Input](./chapter-1-voice-to-action/index.md)
2. [Chapter 2: Cognitive Planning with LLMs](./chapter-2-cognitive-planning/index.md)
3. [Chapter 3: Capstone — The Autonomous Humanoid](./chapter-3-autonomous-humanoid/index.md)

Let's begin by understanding how voice commands are converted into robot actions.