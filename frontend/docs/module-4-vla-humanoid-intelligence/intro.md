# Module 4: Vision-Language-Action (VLA) Humanoid Intelligence

Welcome to Module 4 of the AI-Native Book focused on Physical AI & Humanoid Robotics. This module explores how language, vision, and action are unified in humanoid robots using large language models, speech recognition, and perception systems.

## Overview

This module covers how large language models, speech recognition, and perception systems converge to enable humanoid robots to understand human intent and act autonomously in the physical world:

- **Voice-to-Action**: Converting speech to robot actions using OpenAI Whisper
- **Cognitive Planning**: Using LLMs as reasoning and planning engines
- **Autonomous Behavior**: Complete end-to-end VLA pipeline implementation

## Learning Outcomes

By the end of this module, you should be able to:
- Understand the Vision-Language-Action (VLA) paradigm
- Explain how voice commands are converted into robot actions
- Describe cognitive planning using LLMs
- Conceptually understand end-to-end autonomous humanoid behavior

## Structure

The module is organized into three chapters:

1. [Chapter 1: Voice-to-Action — Speech as Robot Input](./chapter-1-voice-to-action/index.md)
2. [Chapter 2: Cognitive Planning with LLMs](./chapter-2-cognitive-planning/index.md)
3. [Chapter 3: Capstone — The Autonomous Humanoid](./chapter-3-autonomous-humanoid/index.md)

Let's begin by understanding how voice serves as a natural interface for humanoids.